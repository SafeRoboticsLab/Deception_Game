STDOUT will be forked to experiments/bgame_intent_pre_ctrl/v1/log.txt
wandb: Currently logged in as: haiminh. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: haiminh. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: wandb version 0.17.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.2
wandb: Tracking run with wandb version 0.15.2
wandb: Run data is saved locally in /home/haiminh/Codebase/Deception_Game/wandb/run-20240623_173017-9omrl7kb
wandb: Run `wandb offline` to turn off syncing.
wandb: Run data is saved locally in /home/haiminh/Codebase/Deception_Game/wandb/run-20240623_173017-9omrl7kb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run v1
wandb: Syncing run v1
wandb:  View project at https://wandb.ai/haiminh/bgame_intent_pretrain_ctrl
wandb:  View project at https://wandb.ai/haiminh/bgame_intent_pretrain_ctrl
wandb:  View run at https://wandb.ai/haiminh/bgame_intent_pretrain_ctrl/runs/9omrl7kb
wandb:  View run at https://wandb.ai/haiminh/bgame_intent_pretrain_ctrl/runs/9omrl7kb

== Environment information ==
This is a zero-sum environment.
Straight road, box footprint, box obstacles!

== Solver information ==
= Constructing policy agent
The neural networks for CRITIC have the architecture as below:
ModuleList(
  (0): Sequential(
    (linear_1): Linear(in_features=20, out_features=128, bias=True)
    (act_1): ReLU()
  )
  (1-2): 2 x Sequential(
    (linear_1): Linear(in_features=128, out_features=128, bias=True)
    (act_1): ReLU()
  )
  (3): Sequential(
    (linear_1): Linear(in_features=128, out_features=1, bias=True)
    (act_1): Identity()
  )
)
The neural networks for CRITIC have the architecture as below:
ModuleList(
  (0): Sequential(
    (linear_1): Linear(in_features=22, out_features=128, bias=True)
    (act_1): ReLU()
  )
  (1-2): 2 x Sequential(
    (linear_1): Linear(in_features=128, out_features=128, bias=True)
    (act_1): ReLU()
  )
  (3): Sequential(
    (linear_1): Linear(in_features=128, out_features=1, bias=True)
    (act_1): Identity()
  )
)
The neural network for MEAN has the architecture as below:
ModuleList(
  (0): Sequential(
    (linear_1): Linear(in_features=18, out_features=256, bias=True)
    (act_1): ReLU()
  )
  (1-2): 2 x Sequential(
    (linear_1): Linear(in_features=256, out_features=256, bias=True)
    (act_1): ReLU()
  )
  (3): Sequential(
    (linear_1): Linear(in_features=256, out_features=2, bias=True)
    (act_1): Identity()
  )
)

Total parameters in actor: 273924
We want to use: cuda:0, and Agent uses: cuda:0
Critic is using cuda:  True

== Learning starts ==
Traceback (most recent call last):
Traceback (most recent call last):
  File "script/bgame_intent_pretrain_ctrl.py", line 190, in <module>
    main(args.config_file)
  File "script/bgame_intent_pretrain_ctrl.py", line 190, in <module>
    main(args.config_file)
  File "script/bgame_intent_pretrain_ctrl.py", line 172, in main
    solver.learn(env, visualize_callback=visualize_callback)
  File "script/bgame_intent_pretrain_ctrl.py", line 172, in main
    solver.learn(env, visualize_callback=visualize_callback)
  File "/home/haiminh/Codebase/Belief-Game/agent/naive_rl_dr.py", line 88, in learn
    obs = venv.reset_one(index=env_idx)
  File "/home/haiminh/Codebase/Belief-Game/agent/naive_rl_dr.py", line 88, in learn
    obs = venv.reset_one(index=env_idx)
  File "/home/haiminh/Codebase/Belief-Game/simulators/vec_env/vec_env.py", line 58, in reset_one
    obs = self.env_method('reset', indices=[index], **kwargs)[0]
  File "/home/haiminh/Codebase/Belief-Game/simulators/vec_env/vec_env.py", line 58, in reset_one
    obs = self.env_method('reset', indices=[index], **kwargs)[0]
  File "/home/haiminh/Codebase/Belief-Game/simulators/vec_env/subproc_vec_env.py", line 216, in env_method
    return [remote.recv() for remote in target_remotes]
  File "/home/haiminh/Codebase/Belief-Game/simulators/vec_env/subproc_vec_env.py", line 216, in env_method
    return [remote.recv() for remote in target_remotes]
  File "/home/haiminh/Codebase/Belief-Game/simulators/vec_env/subproc_vec_env.py", line 216, in <listcomp>
    return [remote.recv() for remote in target_remotes]
  File "/home/haiminh/Codebase/Belief-Game/simulators/vec_env/subproc_vec_env.py", line 216, in <listcomp>
    return [remote.recv() for remote in target_remotes]
  File "/home/haiminh/Downloads/yes/envs/isaacs/lib/python3.8/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "/home/haiminh/Downloads/yes/envs/isaacs/lib/python3.8/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "/home/haiminh/Downloads/yes/envs/isaacs/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/home/haiminh/Downloads/yes/envs/isaacs/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/home/haiminh/Downloads/yes/envs/isaacs/lib/python3.8/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
  File "/home/haiminh/Downloads/yes/envs/isaacs/lib/python3.8/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
KeyboardInterrupt
